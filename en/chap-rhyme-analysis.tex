
\chapter{Rhyme detection}\label{chap-rhyme-analysis}
In this Chapter, we will first consider using available tools for rhyme detection. After not finding the right tool we will reconsider to make the detection ourselves.

Detecting rhymes may seem like a simple task at first, but looking into details one discovers many problems that need to be addressed. As we have seen in Chapter \ref{chap-related-work}, it is not a well-defined task so we need to set the requirements.

Then we progress through individual steps needed to find the rhymes and design a rating that would evaluate song's rhymes:


\begin{enumerate}
	\item phonetic transcription with additional data preprocessing
	\item syllabification and extraction of phonemes after last stressed syllable
	\item comparison of two lines and calculating their rhyme rating
	\item finding all rhymes and assigning a scheme
	\item calculating song rating
\end{enumerate}

\section{Using available tools for detection}
The simplest approach would be to use one of the tools described in section \ref{rhyme_detection_tools}. We want a detector that is free, strong (detects more than perfect rhymes), and offers headless mode (we could run it automatically from code). Ruling out unsuitable tools, we are left with Rhyme Tagger and SPARSAR. Rhyme Tagger is easy-to-use but only outputs rhyme scheme. To be able to automatically evaluate the rhyming quality of a song, we would need more information like stress or rhyme type.

\subsection*{SPARSAR}
SPARSAR, on the other hand, has a very rich and detailed output. Although it is lacking documentation, the \textit{xml} output format is quite descriptive to understand what most of the values represent. It seemed promising so we attempted to pursue this path.

To bridge the outdated system requirements, we contacted the authors for a newer build (for Ubuntu 19.1). They were very helpful and soon we were able to run it on our computer. Nevertheless, we encountered several issues, mostly stemming from the fact that SPARSAR was written in Prolog. Firstly, the xml output was difficult to parse automatically, as the values were written in Prolog syntax, using the same delimiter for different levels of separation. Secondly, SPARSAR parses the text in sentences but lyrics are usually written without punctuation. We added punctuation using \textit{punctuator2} (\cite{tilk2016punctuation}) which also added space for error. 

Lastly, when SPARSAR encountered an unknown word, it failed for the entire song. Since our data were crowd-sourced, it contained many unusual words, so in the beginning it failed on 80\% of our data. We iteratively worked with the authors for months on fixing bugs and adding words (mainly contractions, such as I'mma, y'all, yo', 'em, etc.) into their dictionary. In the end, we were able to successfully run it on 95\% of our data.

However, we were still not very satisfied with the result. As you can see in the example in Table \ref{sparsar_wrong_scheme}, it failed to detect the perfect rhyme between 2\textsuperscript{nd} and 4\textsuperscript{th} line, and instead marked a rhyme on line 1 and 3, where there was none. The example shows the first four lines of the song, but the scheme letter \textit{h} does repeat only once more on line 34. Such errors were not sparse and led us to believe that the encoded inclination of this tool to look for sonnet-shaped schemes caused it to make errors in the diverse schemes of song lyrics. It may be a great tool for sonnets, but we have found it insufficient for our purpose, so we proceeded to create our own rhyme detector.


% \begin{table}[h!]
% 	\centering
% 	\begin{tabular}{c c c} 
% 		Scheme & Line & \begin{tabular}{@{}c@{}}Last word's pronunciation \\ (as assigned by SPARSAR)\end{tabular}  \\ [0.5ex] 
% 		\midrule
% 		a & Pulled out from the station & s-t-ey-sh-ah-n \\ 
% 		h & fifteen after two & t-uw \\
% 		a &	300 miles away from Vegas & v-ey-g-ah-s\\
% 		b & We had nothin better to do & d-uw\\
% 	\end{tabular}
% 	\caption{Exampleof incorrect scheme assignment by SPARSAR. Excerpt from the song \textit{Good Life}.}
% 	\label{sparsar_wrong_scheme}
% \end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{@{}c c c c@{}}\toprule
		\multicolumn{2}{c}{Scheme} & Line & Last word's pronunciation \\\cmidrule{1-2}
 		correct & SPARSAR & & as assigned by SPARSAR  \\
		\midrule
		A & a & Pulled out from the station & s-t-ey-sh-ah-n \\ 
		B & h & fifteen after two & t-uw \\
		C & a &	300 miles away from Vegas & v-ey-g-ah-s\\
		B & b & We had nothin better to do & d-uw\\
		\bottomrule
	\end{tabular}
	\caption[Example of incorrect scheme assignment by SPARSAR. Excerpt from the song \textit{Good Life}]{Example of incorrect scheme assignment by SPARSAR. Excerpt if the beginning of the song \textit{Good Life}. The correct scheme is marked in capital letters,	the scheme assigned by SPARSAR is marked in lowercase letters. }
\label{sparsar_wrong_scheme}
\end{table}


\section{Defining the requirements}\label{defining_the_requirements}
Before we dive into algorithm selection and implementation details of our detector, let's define what exactly do we want our detector to do. Additionally, we also establish terms for common cases in Table \ref{terms}, to keep our further explanations short and clear.
\begin{table}[h!]
	\centering
	\begin{tabular}{>{\bfseries}l l} 
		component & a vowel or a consonant cluster in a syllable\\
		rhyme candidates & two lines that are being compared for rhyme \\
		rhyming fellows & two lines that rhyme together\\
		rhyming part & the exact components that participate in rhyme \\&(i.e. are equal or similar in sound)\\
		rhyme group & a group of lines that all rhyme together\\& (i.e. have identical scheme letter) \\
		rhyme rating & rating of the quality of one rhyme between two lines\\
		song rating & rating of the rhyming quality of the entire song\\
	\end{tabular}
	\caption{Establishing terms.}
	\label{terms}
\end{table}

\paragraph{Input}
Although we realize sound can affect rhymes, for the purpose of this thesis we will focus solely on text. As \cite{rhymes_overview} points out, most works focus on rhyme schemes in well structured poetry instead of common rhymes in text. We do not have standard rhyme scheme patterns nor fixed syllable counts -- song authors use rhymes arbitrarily. Although in some genres, mainly rap, rhymes in the middle of the line (\gls{internal_rhyme}s) can occur, as we decided in Chapter \ref{chap-related-work} we will only focus on \gls{end_rhyme}s in this thesis. For our input, we expect song lyrics in English, formatted in lines with rhyme always at the end of line. Optional empty lines between stanzas or chorus will be preserved but skipped.

\paragraph{Output}
The main element of our output is rhyme scheme. As a single element, it gives the best overview of the song and more importantly, it allows us to compare our detector with others or with the \gls{gold_data}. Additionally, we want more information to assess rhyme quality: rhyme rating for each individual rhyme, song rating, rhyme type, pronunciation of the rhyming part, optional modification of stress, etc.


\section{Pronunciation}
\subsection{Phonetic alphabets overview}
Unlike many other languages, English does not have a straightforward pronunciation rules. Therefore to be able to assess rhymes, we need to transcribe our text into a phonetic alphabet first. There are two commonly used alphabets to choose from -- IPA and ARPAbet. The original International Phonetic Alphabet (IPA) used since 1888 uses one UNICODE character to encode each phoneme and it is commonly used for example in dictionaries. Since it uses non-ASCII characters, ARPAbet was developed as an equivalent for computers. It has two versions: 1-character that uses upper-case and lower-case letters and (the more common) 2-character version where each phoneme is represented by one or more upper-case ASCII characters (\cite{lea1980trends})(see Table \ref{pronunciation_table} for comparison). We will be using the 2-character ARPAbet because it is used by the CMUdict.

\begin{table}[h!]
	\centering
	\begin{tabular}{c c c c} 
		Example word & IPA & 1-character ARPAbet & 2-character ARPAbet \\ [0.5ex] 
		\hline
		st\textbf{o}ry & \textipa{O} & c & AO \\ 
		bu\textbf{tt}er & \textipa{R} & F & DX \\
	\end{tabular}
	\caption{Short comparison of different pronunciation alphabets.}
	\label{pronunciation_table}
\end{table}
\subsection{CMUdict}\label{cmudict}
Carnegie Mellon University Pronouncing Dictionary (CMUdict) is an open-source pronunciation dictionary.\footnote{\url{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}} Currently it contains 134,373 words (including their inflections) and their pronunciations in 2-character ARPAbet. 
For each word, there is one or several possible pronunciations in North American English including stress markers for primary, secondary or no stress. For the implementation, we used its Python wrapper package \textit{cmudict}.\footnote{\url{https://pypi.org/project/cmudict/}} To use this we had to strip the input of punctuation and convert it to lower case.

CMUdict is a large dictionary and it includes also slang words so it should cover most of our input. To test this, we looked at all last words on each line of our data (since those are the important ones for rhyme analysis) and we found out that 5.52\% of them are not included in CMU dictionary. We manually inspected a small portion of them and found out that they can be mostly split into the following 5 categories:

\begin{itemize}
	\item uncommon words, e.g. superglue, redundantly
	\item misspelled words, e.g. decsion, girlfren
	\item numbers
	\item foreign words, e.g. revoluccion, ecolli
	\item interjections and onomatopoeia, e.g. shoooshooo, woahwoah
\end{itemize}

\subsection{Dealing with out-of-dictionary words}
In an attempt to decrease the amount of out-of-dictionary words, we replaced the closing single quotation mark ``\textipa{’}'' (U+2019) with the typewriter apostrophe ``\texttt{\fontencoding{OT1}\selectfont\symbol{13}}''
 (U+0027) since only the second variant of apostrophe is accepted by CMUdict. However, this only decreased the percentage of unrecognized words to 5.47\%.

Clearly, we needed a way to estimate the pronunciation for the rest of the words, so we used grapheme-to-phoneme library \textit{g2p} by \cite{g2pE2019}. It predicts the pronunciation for out-of-dictionary words using deep learning seq2seq model by TensorFlow (\cite{tensorflow2015-whitepaper}).

Even having the pronunciation for every word will not ensure we find every rhyme intended. Song artists may take their liberty in modifying or skewing the pronunciation to make the rhyme work. Sometimes they can also sing two syllables in one beat or use an unusual pronunciation from different culture to convey a message. As we established in the beginning, we will focus only on information retrievable from the text and ignore these possible deviations in pronunciation.

\section{Syllabification}
\subsection{Alignment}
When comparing lines for rhymes, we have to establish a system of alignment so that we analyze only relevant pairs of phonemes. Initially, we created a simple rhyme detector that just traversed both verses backwards phoneme by phoneme\footnote{For simplification, we use the term phoneme for one symbol in ARPAbet.} and compared them. However, rhyming words do not have to have an equal number of phonemes. For example words in the Table \ref{phon_misalign_table} have a 2-syllable rhyme. If we compared all phonemes one by one they get misaligned on consonant clusters S-T-R and P-L and we will miss the penultimate syllable rhyme. For forced rhymes this inherently becomes a very frequent issue.

\begin{table}[h!]
		\centering
	\begin{tabular}{c r} 
		Word & ARPAbet transcription \\ [0.5ex] 
		\hline
		constrain & K AH N -- S \space\space T R EY N \\ 
		complain & K AH\space  M --  P L EY N \\
	\end{tabular}
	\caption{Example of misalignment when aligning by phonemes.}
	\label{phon_misalign_table}
\end{table}

We need to make sure that we are comparing corresponding parts of verses otherwise we will miss the rhyme. A better approach would be to compare corresponding syllables because they naturally create an alignment in the rhythm of the song. Each syllable can be further split into 3 groups (``CVC'') -- leading consonant cluster (\textit{onset}), vowel (\textit{nucleus}), and trailing consonant cluster (\textit{coda}). Consonant clusters can sometimes be empty. For syllabification we used Python library \textit{syllabify},\footnote{\url{https://github.com/kylebgorman/syllabify}} which requires input in ARPAbet (we use the output of CMUdict) and conveniently returns syllables in CVC triplets as described above.


\subsection{Extracting relevant components}\label{extracting_relevant_phonemes}
Since we established that rhymes are located at the end of each line, there is no need to analyze the entire verse. How far should we look? The first choice would be to look at the last word only. However rhymes can extend over more words as we see in example in Figure \ref{two-word_rhyme}.
\begin{figure}[htb]\centering
	I was the man the Duke \textbf{spoke to};\\
	I helped the Duchess to cast off his \textbf{yoke, too};\\
	\caption{An example of two-word rhyme from \textit{The Flight of the Duchess} by Robert Browning.} 
	\label{two-word_rhyme}
\end{figure} 

When we look at rhyme types, they do not go further then the first stressed syllable (looking at the line backwards). Notably, even if the rhyme does extend further we can ignore the rest because it cannot increase the rating. For example, if there are more rhyming syllables preceding the perfect rhyme, they cannot make the score better. Similarly, if the rhyme is not perfect, syllables preceding the final stress would already be considered an \gls{internal_rhyme} -- which is also used (mainly in rap lyrics) but less valued than the classical end rhyme and as stated in Section \ref{defining_the_requirements}, not a target of analysis in this thesis. We will therefore limit our window to the minimal number of syllables needed to include the stressed syllable in both rhyme fellows. If one rhyme fellow needs less syllables than the other (i.e. it is an imperfect rhyme), we will move stress to the right to match the shorter syllable span (and later decrease the rhyme rating by a \textit{stress penalty, cf. Section~\ref{rhyme-rating}}).


\section{Rhyme analysis}
\subsection{Finding similar phonemes}
Finding perfect rhymes is easy -- phonemes after the last stress need to agree exactly. Imperfect rhymes can be found similarly, using the trick of moving the stress described in Section \ref{extracting_relevant_phonemes}.
To set ourselves apart from simple rhyme detectors, we need to detect more than just perfect and imperfect rhymes -- we need to find forced rhymes as well. To determine forced rhymes we have to assess the match in sound of individual phonemes.

For this, we took inspiration from \cite{plechavc2018collocation}. He used the fact that rhymes tend to reoccur and, having enough data, commonly co-occurring pairs are most probably rhymes even without the knowledge of their pronunciation. He calculated the probabilities of phoneme co-occurrences in line-end words from their frequencies in data, used this co-occurrence matrix to predict rhymes, and then iteratively recalculated the matrix using the EM algorithm.

However, our system of alignment is different so our matrix components will look differently. The differences are shown in Table \ref{alignment}. The first difference is that Plecháč only uses vowels and consonant clusters without acknowledging the syllable separation. We do separate the consonant cluster into two on the border of syllables (e.g. ``rp`` in carpenter is in one cluster for Plecháč, but in two separate clusters for us). The second difference is that Plecháč recognized the position of the component and paired only the components on the same position (i.e. both ``\textipa{@}'' in carpenter are in separate groups because they occur on different positions in the word). We do not believe that position has an effect on phoneme similarity and therefore we add it together for all positions, e.g. if once two phonemes co-occur on 3\textsuperscript{rd} position and once on the 5\textsuperscript{th} we will say that this pair of phonemes has 2 co-occurrences.

\begin{table}[h!]
	\centering
	\begin{tabular}{c r} 
		Plecháč & \textipa{\color{blue}k \color{magenta}A: \color{PineGreen} r.p \space\space \color{BurntOrange} @ \color{BrickRed}  n.t \space\space\color{Cerulean} @ \color{Fuchsia}r}\\
		Us & \textipa{\color{blue}k \color{magenta} A: \color{blue}r \space .p \color{magenta} @ \color{blue} n \space .t \color{magenta} @ \color{blue}r} \\
	\end{tabular}
	\caption[Comparison of alignments]{Comparison of our components for word \textit{carpenter} with those of \cite{plechavc2018collocation}. Different color signifies different component group.} 
	\label{alignment}
\end{table}

Another difference is that Plecháč only looks at the last word and its 1 or 2 last syllables while we look at everything after the last stress up to 4 syllables.

Knowing that perfect match in sound always means perfect rhyme (which should intuitivelly get the highest possible rhyme rating of 1.0), we froze the values on matrix diagonal to reflect it. In \cite{plechavc2018collocation}, the diagonal was being recalculated as the rest of the matrix, and although it seemed to converge to high numbers we felt it did not reflect the superiority of the perfect match.

To calculate the probabilities in the matrix, we used the formula from \cite{plechavc2018collocation} adding the adjustments described above. The formula for a the value in the matrix on coordinates i, j is as follows: 


	    \[ p(i,j) = \begin{cases} 
	    \mbox{1.0,} & \mbox{if  i=j} \\ 
	  \dfrac{f(i,j)}{f(i,j) + f(i)f(j)},
	     & \mbox{otherwise} \\
	    \end{cases} \]

where\textit{ f(i,j)} represents the frequency of the co-occurrence of the pair of components \textit{i} and \textit{j}, and similarly \textit{f(i)} represents the total number of occurrences of the component \textit{i}.
\MP{Ale Plecháč definuje f(i,j) jako \textbf{relative} frequency, tedy pravděpodobnost souvýskytu, nikoli absolutní počet souvýskytů.
Obdobně f(i) = count(i) / total\_count. Kde $\sum_i f(i) = total\_count$.
Je-li to jen nepřesnost v textu, tak jen doplňte k frequency přívlastek relative a nahraďte ten total number of occurrences.
Je-li v kódu taky absolutní počet, tak nevím -- opravit to už zjevně není čas.
Omlouvám se, že jsem si toho všiml až teď.
}
\MP{Ještě by chtělo lépe definovat, co je occurrence -- tedy výskyt dané komponenty na konci řádku po posledním přízvuku.
Započítáváte do toho i výskyty, kde odpovídající komponenta v rhyming fellow je identická?}
\MP{I ty co-occurerences by bylo dobré lépe definovat -- je to souvýskyt na odpovídají pozici v rhyming fellow, přičemž je k tomu tedy potřeba mít nějak určené rýmy,
abychom věděli, který z předchozích řádků je ten fellow, což máte vysvětlené v následující sekci.}

Technically, the matrix is only triangular because the order of \textit{i} and \textit{j} does not matter.

\subsection{Training the detector} \label{analysis:training}
What we have just described is basically step 3 of learning the matrix using the EM algorithm. The whole outline can be summed up in 4 steps:
\begin{enumerate}
	\item Initialize the matrix of vertical collocation probabilities.
	\item Find rhymes using this matrix.
	\item Adjust the matrix based on detected rhymes.
	\item Repeat steps 2 and 3 until convergence.
\end{enumerate} 

To improve the matrix in step 3, in each iteration we will count the vertical collocations only from the pairs that were marked as rhyming in the previous iteration.


For the matrix initialization, Plecháč calculates the vertical collocations in the data and preserves only pairs with number of vertical collocations above a set threshold. We instead initialized it with the default value of 0.2. 

Finding rhymes (step 2) is further described in Section \ref{finding-all-rhymes}.


\subsection{Rhyme rating}\label{rhyme-rating}
 We will first calculate rhyme ratings for pairs of verses and then use them to calculate an overall song rating. Not only do these rhyme ratings help us evaluate the rhyming quality of the song but they might also be an interesting feedback for the users (e.g. authors of song lyrics) -- we thus show the rhyme ratings in our web visualization as described in Section~\ref{visualization-matrix}.
 
 
 For each rhyme, we would like to assign a rating between 0 and 1. Since perfect rhyme is often in literature described as superior because it represents the perfect match in sound it will logically receive the highest rating of 1.
 
 For other cases, such as imperfect rhymes or some forced rhymes, where it was necessary to move the stress to the rhyming part, we will give a penalty of -0.1 for moving the stress. This includes both the case when stress on one line had to be moved to match the other, and the case when stress on both lines had to be moved to exclude non rhyming syllables from the rhyme.
  
	When the phoneme sounds are similar, we will assign rhyme rating as a simple multiplication of probabilities for the individual components from the matrix. 

This is the final formula
	
	\[rhyme\ rating = stress\_penalty + \prod_{i,j\ \epsilon\ rhyming\ part} p(i,j) \]
	
where \textit{stress\_penalty} is 0 if stress was not moved, -0.1 otherwise. Indices \textit{i, j} iterate over components in rhyming parts of the both words.





\section{Scheme}
\subsection{Finding all rhymes}\label{finding-all-rhymes}
To search for rhymes in the full lyrics, we need to decide which verse pairs to check. The most straight-forward approach would be ``brute force'' -- try each line with all the other lines. Besides its obvious disadvantage of increased time requirements it also detects rhymes that span across tens of lines. It is not strictly defined how many lines apart can the rhyme fellows be to still be considered a rhyme -- the author can even make it a part of his artistic expression, e.g. in \textit{Author's Prologue} by \cite{thomas1952author} the 1\textsuperscript{st} line rhymes with 102\textsuperscript{th}, 2\textsuperscript{nd} with 101\textsuperscript{th} and so on. Realistically, a rhyme between a line in the beginning of the lyrics and 20 lines later, would hardly be noticed at all by song's listeners -- it requires a close proximity of rhyme fellows within the poem. We decided to set the default window size to 5, to include support for rhyme propagation within stanzas.


We traversed the song lyrics from beginning to the end, line by line, for each line trying all rhyme candidates in the given window.

Some words may have multiple possible pronunciations -- in that case we evaluate each possible combination of pronunciations for all words in the given pair. For every such combination we assign rhyme rating and create a list of candidates.

As \cite{rhymes_overview} states, it is difficult to know where to draw the line between intended rhyme and accidental word similarity. However, a threshold must be set to draw a boundary between the two. By default, we have set this threshold to 0.8 because we found it to work quite well in our data. However, it can be adjusted by users to their value of choice; similarly with other parameters such as window size. The list of rhyme candidates can now be reduced to the ones with rhyme rating above the selected threshold.

From this list of candidates for each line, we select only the candidate with the highest rhyme rating. When the ratings are identical, we select the closest line. Other candidates are saved, in case changes need to be made later in scheme adjustment phase, cf. Section~\ref{sec:scheme-adjustment}. When the line does not have any suitable rhyme candidates, it is assigned rating 0. If the line rhymes with a candidate that is already a part of a rhyme, they join together into a rhyme group of 3 (or more) lines.
 



\subsection{Assigning rhyme scheme}\label{sec:scheme}
 Rhymes in songs or poems are typically marked using a rhyme scheme. That means each verse gets assigned a letter -- lines that share the same letter rhyme and those with different letters do not. We also decided to adapt this common notation. In case the song needs more letters than there are in the alphabet, we will add another letter and continue alphabetically -- a, b, c, ..., aa, ab, ac, ..., ba, bb, bc, ..., ca, etc.
 
 There are two options for representing non-rhyming lines. The first is to assign every non-rhyming line the same default character. We chose this option as the default (using character ``--''), because it is easier to read for the user. The second option is to assign each non-rhyming line a unique rhyme scheme letter. This approach is more suitable for metrics that look at rhyme scheme letters as clusters of rhyming words. We support both options and can convert one into the other when needed.

\subsection{Scheme adjustment}\label{sec:scheme-adjustment}
In some cases, the algorithm of selecting the best rhyme for each line does not yield the best possible scheme. Consider, for illustration, the example in Table \ref{scheme_adjustment}. There is a perfect rhyme between lines 1-2 and 3-4, and a forced rhyme between lines 2-3. With the algorithm as is, it would receive the \textit{aaaa} scheme. However, that is not what a human, for example a gold data annotator, would assign. He would see that similarity inside the first and the second tuple is greater so they logically form two rhyme groups and the scheme is \textit{aabb}. 

Additionally, song rating (as defined in the following section~\ref{sec:song-rating}) for scheme \textit{aaaa} would be less than 1 (because of the lower rating of the forced rhyme between lines 2 and 3) while the song rating for \textit{aabb} would be equal to 1. Loosing rating by marking weaker rhymes does not make sense, so we must add an exception to only keep the better score. We can see that this problem is similar to the problem of maximizing song rating. 

\begin{table}
	\begin{tabular}{l l}
		1&Packs of Backwoods and Dutches, leave the Swishers for the \textbf{sweeties}  \\
		2&Only roaches in the dishes we be ripping up your \textbf{beedies}  \\
		3&We be ripping up your treaties, I ain't ripping if it's \textbf{seedy}  \\
		4&I ain't riffing, I ain't raffing, I'm just rapping on a \textbf{CD} \\
	\end{tabular}
	\caption[Scheme adjustment example.]{Example of scheme adjustment from \textit{aaaa} to \textit{aabb}. Excerpt from the song \textit{Whooping Cough} from  our dataset.}
	\label{scheme_adjustment}
\end{table}

To address this issue, we have to do one more iteration over the resulting rhyme groups. We focused only on rhyme groups of size 4 and larger because for smaller group such changes would not increase the score. For a larger group, we iterate over the rhymes, starting from the ones with the lowest rating, and try how would removing this rhyme affect the song rating. If it increased the song rating, we keep the change and split the rhyme group as necessary. If the rating did not increase, we try removing the next rhyme, and if we ran out of rhymes to remove, we keep the group as is.

\section{Calculating song rating}\label{sec:song-rating}
The next step is to combine these rhyme ratings into one final rating for the entire song. We will use the straight-forward approach of averaging the assigned rhyme ratings. The dilemma here is where to store the rhyme rating since rhyme is a property of two lines. The first logical idea would be to store it with each line participating in the rhyme. However, then we would add it to the final song rating twice. Moreover, for larger rhyme groups, it would be disproportionate, because third and all the following lines would be added only once. 

Therefore, we decided to store the rating only with the second line of the rhyme. That means the first line in each rhyme group will always be assigned the default value ``-''. It cannot be assigned rhyme rating 0 because that would mean it is a non-rhyming line and would lower the final average. In summary, song rating is the average of rhyme ratings for all rhymes except the lines with unassigned rating ``-''.
