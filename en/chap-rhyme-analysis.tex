
\chapter{Rhyme detection}
Detecting rhymes may seem like a simple task at first, but looking into details one discovers many problems that need to be addressed. As we have seen in chapter \ref{chap-related-work}, it is not a well-defined task so boundaries need to be set.
\todo[inline]{sucast zadania ze to bude bez zvukovej stranky hoci to vplyv ma}
\todo[inline]{pri popise taggeru dat typy rymov, ktore nezvlada}


\footnote{\url{http://phylonetworks.blogspot.com/2020/07/automated-detection-of-rhymes-in-texts.html}}
- points out most works focus on rhyme schemes in well structured poetry instead of common rhymes in text
- where to draw the line between intended rhyme and accidental word similarity

\begin{enumerate}
	\item phonetic transcription with additional data preprocessing	
	\item syllabification and extraction of phonemes after last stressed syllable
	\item comparison of two lines and calculating their rhyme rating  
	\item finding all rhymes and assigning scheme
	\item calculating song rating
\end{enumerate}

\section{Using available tools for detection}
The simplest approach would be to use one of the tools described in section \ref{rhyme_detection_tools}. We want a detector that is free, strong (detects more than perfect rhymes), and offers headless mode (we could run it automatically from code). Ruling out unsuitable tools, we are left with Rhyme Tagger and SPARSAR. Rhyme Tagger is easy-to-use but only outputs rhyme scheme. To be able to automatically evaluate the rhyming quality of a song, we would need more information like stress or rhyme type.

\subsection*{SPARSAR}
SPARSAR, on the other hand, has a very rich and detailed output. Although it is lacking documentation, the \textit{xml} output format is quite descriptive to understand what most of the values represent. It seemed promising so we attempted to pursue this path.

To bridge the outdated system requirements, we contacted the authors for a newer build (for Ubuntu 19.1). They were very helpful and soon we were able to run it on our computer. Nevertheless, we encountered several issues, mostly stemming from the fact that SPARSAR was written in Prolog. Firstly, the xml output was difficult to parse, as the values were written in Prolog syntax, using the same delimiter for different levels of separation. Secondly, SPARSAR parses the text in sentences but lyrics are usually written without punctuation. We added punctuation using \textit{punctuator2} (\cite{tilk2016punctuation}) which also added space for error. 

Lastly, when SPARSAR encountered an unknown word, it failed for the entire song. Since our data was crowd-sourced, in contained many unusual words, so in the beginning it failed on 80\% of our data. We iteratively worked with the authors for months on fixing bugs and adding words (mainly contractions, such as I'mma, y'all, yo', 'em, etc.) into their dictionary. In the end, we were able to successfully run it on 95\% of our data.

However, we were still not very satisfied with the result. As you can see in the example in Table \ref{sparsar_wrong_scheme}, it failed to detect the perfect rhyme between 2\textsuperscript{nd} and 4\textsuperscript{th} line, and instead marked a rhyme on line 1 and 3, where there was none. Such errors were not sparse and led us to believe, that the encoded inclination of this tool to look for sonnet-shaped schemes caused it to make errors in the diverse schemes of song lyrics. It may be a great tool for sonnets, but we have found it insufficient for our purpose, so we proceeded to create our own rhyme detector.


\begin{table}[h!]
	\centering
	\begin{tabular}{c c c} 
		Scheme & Line & \begin{tabular}{@{}c@{}}Last word's pronunciation \\ (as assigned by SPARSAR)\end{tabular}  \\ [0.5ex] 
		\hline
		a & Pulled out from the station & s-t-ey-sh-ah-n \\ 
		h & fifteen after two & t-uw \\
		a &	300 miles away from Vegas & v-ey-g-ah-s\\
		b & We had nothin better to do & d-uw\\
	\end{tabular}
	\caption{Example of incorrect scheme assignment by SPARSAR. Excerpt from the song \textit{Good Life}.}
	\label{sparsar_wrong_scheme}
\end{table}

\section{Defining the requirements}\label{defining_the_requirements}
Before we dive into algorithm selection and implementation details of our detector, let's define what exactly do we want our detector to do. Additionally, we also establish terms for common cases to keep our further explanations short and clear.
\begin{table}[h!]
	\centering
	\begin{tabular}{>{\bfseries}l l} 
		component & a vowel or a consonant cluster of a syllable\\
		rhyme candidates & two lines that are being compared for rhyme \\
		rhyming fellows & two lines that rhyme together\\
		rhyming part & the exact components that participate in rhyme \\&(are equal or similar in sound)\\
		rhyme group & a group of lines that all rhyme together\\& (have identical scheme letter) \\
		rhyme rating & rating of the quality of one rhyme between two lines\\
		song rating & rating of the rhyming quality of the entire song\\
	\end{tabular}
	\label{terms}
\end{table}

\paragraph{Input}
For our input, we expect song lyrics in English, formatted in lines with rhyme always at the end of line. If there will be rhyme in the middle of the line, it will be considered an \textit{\gls{internal_rhyme}} and will not be detected, as we decided in chapter \ref{chap-related-work} to only focus on end rhymes in this thesis. Optional empty lines between stanzas or chorus will be preserved but skipped.

\paragraph{Output}
The main element of our output is rhyme scheme. As a single element, it gives the best overview of the song and more importantly, it allows us to compare our detector with others or with the \gls{gold_data}. Additionally, we want more information to assess rhyme quality: rhyme rating for each individual rhyme, song rating, rhyme type, pronunciation of the rhyming part, optional modification of stress, etc.


\section{Pronunciation}
\subsection{Phonetic alphabets overview}
Unlike many other languages, English does not have a straightforward pronunciation rules. Therefore to be able to assess rhymes, we need to transcribe our text into a phonetic alphabet first. There are two commonly used alphabets to choose from -- IPA and ARPAbet. The original International Phonetic Alphabet (IPA) used since 1888 uses one UNICODE character to encode each phoneme and it is commonly used for example in dictionaries. Since it uses non-ASCII characters, ARPAbet was developed as an equivalent for computers. It has two versions: 1-character that uses upper-case and lower-case letters and (the more common) 2-character version where each phoneme is represented by one or more upper-case ASCII characters (\cite{lea1980trends})(see Table \ref{pronunciation_table} for comparison). We will be using the 2-character ARPAbet because it is used by the CMUdict.

\begin{table}[h!]
	\centering
	\begin{tabular}{c c c c} 
		Example word & IPA & 1-character ARPAbet & 2-character ARPAbet \\ [0.5ex] 
		\hline
		st\textbf{o}ry & \textipa{O} & c & AO \\ 
		bu\textbf{tt}er & \textipa{R} & F & DX \\
	\end{tabular}
	\caption{Short comparison of different pronunciation alphabets.}
	\label{pronunciation_table}
\end{table}
\subsection{CMUdict}
Carnegie Mellon University Pronouncing Dictionary (CMUdict) is an open-source pronunciation dictionary.\footnote{\url{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}} Currently it contains 134,373 words (including their inflections) and their pronunciations in 2-character ARPAbet. 
For each word, there is one or several possible pronunciations in North American English including stress markers for primary, secondary or no stress. For the implementation, we used its Python wrapper package \textit{cmudict} \footnote{\url{https://pypi.org/project/cmudict/}}. To use this we had to strip the input of punctuation and convert it to lower case.

CMUdict is a large dictionary and it includes also slang words so it should cover most of our input. To test this, we looked at all last words on each line of our data (since those are the important ones for rhyme analysis) and we found out that 5.52\% of them are not in CMU dictionary. We manually inspected a small portion of them and found out that they can be mostly split into these 5 categories:

\begin{itemize}
	\item uncommon words, e.g. superglue, redundantly
	\item misspelled words, e.g. decsion, girlfren
	\item numbers
	\item foreign words, e.g. revoluccion, ecolli
	\item interjections and onomatopoeia, e.g. shoooshooo, woahwoah
\end{itemize}

\subsection{Dealing with out-of-dictionary words}
In an attempt to decrease the amount of out-of-dictionary words, we replaced the closing quotation mark "\textipa{â€™}" (U+2019) with the typewriter apostrophe "\texttt{\fontencoding{OT1}\selectfont\symbol{13}}"
 (U+0027) since only the second variant of apostrophe is accepted by CMUdict. However, this only decreased the percentage of unrecognized words to 5.47\%.

Clearly, we needed a way to estimate the pronunciation for these words, so we used grapheme-to-phoneme library \textit{g2p} by \cite{g2pE2019}. It predicts the pronunciation for out-of-dictionary words using deep learning seq2seq model by TensorFlow (\cite{tensorflow2015-whitepaper}).

Even having the pronunciation for every word will not ensure we find every rhyme intended. Song artists may take their liberty in modifying or skewing the pronunciation to make the rhyme work. Sometimes they can also sing two syllables in one beat or use an unusual pronunciation from different culture to convey a message. As we established in the beginning, we will focus only on information retrievable from the text and ignore these possible deviations in pronunciation.

\section{Syllabification}
When comparing lines for rhymes, we have to establish a system of alignment so that we analyze only relevant pairs of phonemes. Initially, we created a simple rhyme detector that just traversed both verses backwards phoneme by phoneme\footnote{For simplification, we use the term phoneme for one symbol in ARPAbet.} and compared them. However, rhyming words do not have to have an equal number of phonemes. For example words in the Table \ref{phon_misalign_table} have a 2-syllable rhyme. If we compared each phonemes one by one they get misaligned on consonant clusters S-T-R and P-L and we will miss the second syllable rhyme. For forced rhymes this inherently becomes a very frequent issue.

\begin{table}[h!]
		\centering
	\begin{tabular}{c r} 
		Word & ARPAbet transcription \\ [0.5ex] 
		\hline
		constrain & K AH N - S \space\space T R EY N \\ 
		complain & K AH\space  M -  P L EY N \\
	\end{tabular}
	\caption{Example of misalignment when aligning by phonemes.}
	\label{phon_misalign_table}
\end{table}

We need to make sure that we are comparing corresponding parts of verses otherwise we will miss the rhyme. A better approach would be to compare corresponding syllables. Each syllable can be further split into 3 groups (``CVC'') -- leading consonant cluster (\textit{onset}), vowel (\textit{nucleus}), and trailing consonant cluster (\textit{coda}). Consonant clusters can sometimes be empty. For syllabification we used Python library \textit{syllabify},\footnote{\url{https://github.com/kylebgorman/syllabify}} which conveniently returns syllables in CVC triplets as described above.



\section{Rhyme analysis}


\subsection{Extracting relevant components}
Rhymes are located at the end of each line so there is no need to analyze the entire verse. How far should we look? The first choice would be to look at the last word. However rhymes can extend over more words as we see in Figure \ref{two-word_rhyme}.
\begin{figure}[htb]\centering
	I was the man the Duke \textbf{spoke to};\\
	I helped the Duchess to cast off his \textbf{yoke, too};\\
	\caption{An example of two-word rhyme from \textit{The Flight of the Duchess} by Robert Browning.} 
	\label{two-word_rhyme}
\end{figure} 

When we look at our rhyme types, they do not go further then the first stressed syllable (looking at the line backwards). Notably, even if the rhyme does extend further we can ignore the rest because it cannot increase the rating. For example, if there are more rhyming syllables preceding the perfect rhyme, they cannot make the score better. Similarly, if the rhyme is not perfect, syllables preceding the final stress would already be considered an \gls{internal_rhyme} -- which is also used (mainly in rap lyrics) but less valued than the classical end rhyme and as stated in section \ref{defining_the_requirements}, not a target of analysis in this thesis. We will therefore limit our window to the minimal number of syllables needed to include the stressed syllable in both rhyme fellows. If one rhyme fellow needs less syllable than other, we will adjust the stress to the right to match the shorter syllable span (and later compensate for this in the rating).


\subsection{Finding similar phonemes}
To set ourselves apart from simple rhyme detectors, we need to detect more than just perfect and imperfect rhymes -- we need to find forced rhymes as well. To determine forced rhyme we need to assess the match in sound of individual phonemes.

For this, we took inspiration from \cite{plechavc2018collocation}. He used the fact that rhymes tend to reoccur and, having enough data, commonly co-occurring pairs are most probably rhymes even without the knowledge of their pronunciation. He calculated the probabilities of phoneme co-occurrences in line-end words from their frequencies in data, used this matrix to predict rhymes, and then iteratively recalculated the matrix using EM algorithm. 

However, our system of alignment is different so our matrix components will look differently. The differences are shown in Table \ref{alignment}. The first difference is that Plechac only has vowels and consonant clusters without acknowledging the syllable separation. We do separate the consonant cluster into two on the border of syllable. The second difference is that Plechac recognized the position of the component and pairs only the components on the same position (shown as color). We do not believe that position has an effect on phoneme similarity and therefore we add it together for all positions, e.g. is once two phonemes co-occur on 3\textsuperscript{rd} position and once on the 5\textsuperscript{th} we will count that this pair of phonemes has 2 co-occurrences.

\begin{table}[h!]
	\centering
	\begin{tabular}{c r} 
		Plechac & \textipa{\color{blue}k \color{magenta}A: \color{PineGreen} r.p \space\space \color{BurntOrange} @ \color{BrickRed}  n.t \space\space\color{Cerulean} @ \color{Fuchsia}r}\\
		Ours & \textipa{\color{blue}k \color{magenta} A: \color{blue}r \space .p \color{magenta} @ \color{blue} n \space .t \color{magenta} @ \color{blue}r} \\
	\end{tabular}
	\caption[Comparison of alignments]{Comparison of our components for word \textit{carpenter} with those of \cite{plechavc2018collocation}. Different color signifies different component group.} 
	\label{alignment}
\end{table}

Another difference is that Plechac only looks at the last word and 1 to 2 syllables while we look at everything after the last stress up to 4 syllables.

Knowing that perfect match in sound always means perfect rhyme, we froze the values on matrix diagonal to reflect it. In \cite{plechavc2018collocation}, the diagonal was being recalculated as the rest of the matrix, and although it converged to high numbers we felt it did not reflect the superiority of the perfect match.

To calculate the probabilities in the matrix, we used the formula from \cite{plechavc2018collocation} adding the adjustments described above. The formula is following: 


	    \[ p(i,j) = \begin{cases} 
	    \mbox{1.0,} & \mbox{if}  i=j \\ 
	    \mbox{...,} 
	     & \mbox{otherwise} \\
	    \end{cases} \]

where\textit{ f(i,j)} represents the frequency of the co-occurrence of the pair of components \textit{i} and \textit{j}, and similarly \textit{f(i)} represents the total number of occurrences of the component \textit{i}. \textit{p(i,j)} is then the value in the matrix on coordinates i, j.


\subsection{Rhyme rating}
 We will first calculate rhyme ratings for pairs of verses and then use them to calculate an overall song rating. Not only do these rhyme ratings help us evaluate the rhyming quality of the song but they might also be an interesting feedback for the writer. 
 
 For each rhyme, we would like to assign a rating between 0 and 1. Since perfect rhyme is often in literature described as superior because it represents the perfect match in sound it will logically receive the highest rating of 1.
 
 For the cases, such as imperfect rhymes or some forced rhymes, where it was necessary to move the stress to the rhyming part, we will give a penalty of -0.1 for this imperfection. 
 
	When the phoneme sounds are similar, we will assign rhyme rating as a simple multiplication of probabilities for the individual components from the matrix.
	
	\[rhyme\ rating = \prod_{i,j\ \epsilon\ rhyming\ part} p(i,j) \]


\section{Training the detector}
Since we are using the EM algorithm to train our detector for recognizing similar phonemes, the outline of the algorithm is as follows:
\begin{enumerate}
	\item Initialize the matrix of collocation probabilities.
	\item Find rhymes using this matrix.
	\item Adjust the matrix based on detected rhymes.
	\item Repeat steps 2 and 3 until convergence.
\end{enumerate} 

For the matrix initialization, Plechac calculates the collocations in the data and preserves only pairs with number of collocations above a set threshold. We instead initialized it with default value of 0.2. 

\paragraph{}

\section{Scheme}
\subsection{Finding all rhymes}
To search for rhymes in the full lyrics we need to decide which verse pairs to check. The most straight-forward approach would be "brute force" -- try each line with all the other lines. Besides its obvious disadvantage of increased time requirements it also detects rhymes that span across tens of lines. It is not strictly defined how many lines apart can the rhyme fellows be to still be considered a rhyme -- the author can even make it a part of his artistic expression  e.g. in "Author's Prologue" by \cite{thomas1952author} the 1\textsuperscript{st} line rhymes with 102\textsuperscript{th}, 2\textsuperscript{nd} with 101\textsuperscript{th} and so on. Realistically, a rhyme between a line at the beginning of the lyrics and 20 lines later would not have a strong effect on the song listener -- it requires a close proximity of rhyme fellows within the poem for the rhyme to be noticed by ear. Since the most common stanzaic form in English is a quatrain, a stanza of four lines (\cite{eastman1970norton}), we decided to set the default window size to 5. 

We traversed the song lyrics from beginning to the end, line by line, for each line trying all rhyme candidates in the given window.

Some words may have multiple possible pronunciations -- in that case we evaluate each possible combination of pronunciations for all words in the given line pair. For every such combination we assign rhyme rating and keep only those with rhyme rating above the selected threshold. For default, we have set this threshold to 0.8 because we found it to work quite well in our data. However, it can be adjusted by user to their value of choice, similarly with other parameters such as window size.  

From this list of candidates for each line, we select only the candidate with the highest rhyme rating. When the ratings are identical, we select the closest line. Other candidates are saved, in case changes need to be made later in scheme adjustment phase. When the line doesn't have any suitable rhyme candidates, it is assigned rating 0. If the line rhyme with a candidate that is already a part of a rhyme, they join together into a rhyme group of 3 (or more) lines.
 



\subsection{Assigning rhyme scheme}
 Rhymes in songs or poems are typically marked using a rhyme scheme. That means each verse gets assigned a letter -- lines that share the same letter rhyme and those with different letters do not. We also decided to adapt this common notation. In case the song needs more letters than there are in the alphabet we will add another letter and continue alphabetically -- a, b, c, ..., aa, ab, ac, ..., ba, bb, bc, ..., ca, etc.
 
 There are two options for representing non-rhyming lines. The first is to assign every non-rhyming line the same default character. We chose this option as the default, because it is easier to read for the user. The second option is to assign each non-rhyming line a unique rhyme scheme letter. This approach is more suitable for metrics that look at rhyme scheme letters as clusters of rhyming words. We support both options and can convert one the other when needed.

\subsection{Scheme adjustment}
In some cases, the algorithm of selecting the best rhyme for each line does not yield the best possible scheme. Consider, for illustration, the example in Table \ref{scheme_adjustment}. There is a perfect rhyme between lines 1-2 and 3-4, and a forced rhyme between lines 2-3. With the algorithm as is, it would receive \textit{aaaa} scheme. However, that is not what a human, for example the gold data annotator, would assign. He would see that similarity inside the first and the second tuple is greater so they logically form two rhyme groups and the scheme is \textit{aabb}. 

Additionally, song rating for scheme \textit{aaaa} would be less than 1 (because of the forced rhyme rating) while the song rating for \textit{aabb} would be equal to 1. Loosing rating by marking weaker rhymes does not make sense so we must add an exception to only keep the better score. We can see that this problem is similar to the problem of maximizing song rating. 

\begin{table}
	\begin{tabular}{l l}
		1&Packs of Backwoods and Dutches, leave the Swishers for the \textbf{sweeties}  \\
		2&Only roaches in the dishes we be ripping up your \textbf{beedies}  \\
		3&We be ripping up your treaties, I ain't ripping if it's \textbf{seedy}  \\
		4&I ain't riffing, I ain't raffing, I'm just rapping on a \textbf{CD} \\
	\end{tabular}
	\caption[Scheme adjustment example.]{Example for scheme adjustment from \textit{aaaa} to \textit{aabb}. Excerpt from the song \textit{Whooping Cough} from  our dataset.}
	\label{scheme_adjustment}
\end{table}

To address this issue, we have to do one more iteration over the resulting rhyme groups. We focused only on rhyme groups of size 4 and larger because for smaller group such changes would not increase the score. For a larger group, we iterate over the rhymes, starting from the ones with the lowest rating, and tried how would removing this rhyme affect the song rating. If it increased the song rating, we kept the change and split the rhyme group as necessary. If the rating did not increase, we tried removing the next rhyme, and if we ran out of rhymes to remove we kept the group as is.

\section{Calculating song rating}
The next step is to combine these rhyme ratings into one final rating for the entire song. We will use the straight-forward approach of averaging the assigned rhyme ratings. The dilemma here is where to store the rhyme rating since rhyme is a property of two lines. The first logical idea would be to store it with each line participating in the rhyme. However, then we would add it to the final song rating twice. Moreover, for larger rhyme groups, it would be disproportionate, because third and all the following lines would be added only once. 

Therefore we decided to store the rating only with the second line of the rhyme. That means the first line in each rhyme group will always be assigned the default value "-". It cannot be assigned rhyme rating 0 because that would mean it is a non-rhyming line and would lower the final average. In summary, song rating is the average of rhyme ratings for all rhymes except the lines with unassigned rating "-".






	
\section{Syllables}
\todo[inline]{check if this wouldn't work with g2p}
Since meter plays an important role in rhymes, another relevant property to examine is the number of syllables. To count syllables for each line we used Python package \textit{syllabify} \footnote{\url{https://github.com/kylebgorman/syllabify}} which returns syllables using ARPAbet transcription. For words not in CMUdict we used a simple heuristic -- since the nucleus of each syllable is most often a vowel (except for syllabic consonants) we counted the number of (groups of) vowels and used it as an estimation for the number of syllables. Although this gives a wrong estimate for some words e.g. \textit{rhythm} or \textit{house}, it performed quite well when we tested it on a few out-of-dictionary words from our dataset. We found it unnecessary to try to further improve the heuristic because the words that are not in CMUdict are often foreign words that do not follow the standard pronunciation rules of English so any application of these rules would probably be of little help.

